{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fullpipline.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "zh_cBw5dT5wm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SpcU3a70UaFl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive  -o nonempty"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BsiXhPMHUsi1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('drive/bertlab')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "303QeQjsUwGV",
        "colab_type": "code",
        "outputId": "249e223a-ca48-41f6-f12c-eb1981cdc7e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "attention.py\t\t    modeling_test.py\n",
            "bert\t\t\t    multilingual.md\n",
            "bertbert.py\t\t    optimization.py\n",
            "bertcode\t\t    optimization_test.py\n",
            "berteval\t\t    position.py\n",
            "Bert-lab\t\t    predicting_movie_reviews_with_bert_on_tf_hub.ipynb\n",
            "BERTmodel\t\t    __pycache__\n",
            "CONTRIBUTING.md\t\t    README.md\n",
            "crazy.py\t\t    requirements.txt\n",
            "create_pretraining_data.py  run_classifier.py\n",
            "download_glue_data.py\t    run_classifier_with_tfhub.py\n",
            "extract_features.py\t    run_pretraining.py\n",
            "feature\t\t\t    run_squad.py\n",
            "forbert\t\t\t    sample_text.txt\n",
            "__init__.py\t\t    self_attention.py\n",
            "labresult\t\t    tokenization.py\n",
            "LICENSE\t\t\t    tokenization_test.py\n",
            "modeling.py\t\t    version2.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fIB_E24aUyUN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -rf bert\n",
        "!git clone https://github.com/google-research/bert"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qSK6Gb_rU5mN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]='2' \n",
        "import sys\n",
        "sys.path.append('bert/')\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import codecs\n",
        "import collections\n",
        "import json\n",
        "import re\n",
        "import pprint\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import modeling\n",
        "import tokenization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JbKxD8xFZFeF",
        "colab_type": "code",
        "outputId": "06b19ce3-f6d9-41a3-a617-ffb7aaab1dc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "cell_type": "code",
      "source": [
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "with tf.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  #with open('/content/adc.json', 'r') as f:\n",
        "    #auth_info = json.load(f)\n",
        "  #tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address is grpc://10.13.153.170:8470\n",
            "TPU devices:\n",
            "[_DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:CPU:0, CPU, -1, 13087122525894256119),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 17179869184, 6443570994248375968),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:0, TPU, 17179869184, 12185343637250955775),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:1, TPU, 17179869184, 5185758515095632824),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:2, TPU, 17179869184, 5249474612802196368),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:3, TPU, 17179869184, 5618969122202604831),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:4, TPU, 17179869184, 13765678340434986615),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:5, TPU, 17179869184, 13193067646207769754),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:6, TPU, 17179869184, 16407866074275330605),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU:7, TPU, 17179869184, 3060932641765872895),\n",
            " _DeviceAttributes(/job:tpu_worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 17179869184, 671512260742070309)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lUukHfTGbWC7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Available pretrained model checkpoints:\n",
        "#   uncased_L-12_H-768_A-12: uncased BERT base model\n",
        "#   uncased_L-24_H-1024_A-16: uncased BERT large model\n",
        "#   cased_L-12_H-768_A-12: cased BERT large model\n",
        "BERT_MODEL = 'chinese_L-12_H-768_A-12' #@param {type:\"string\"}\n",
        "BERT_PRETRAINED_DIR = 'gs://***/' + BERT_MODEL\n",
        "print('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))\n",
        "!gsutil ls $BERT_PRETRAINED_DIR"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UWqydOfqdaWt",
        "colab_type": "code",
        "outputId": "25b13f1d-3bd3-48fd-d7df-82a9fa6e68cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "cell_type": "code",
      "source": [
        "# Available pretrained model checkpoints:\n",
        "#   uncased_L-12_H-768_A-12: uncased BERT base model\n",
        "#   uncased_L-24_H-1024_A-16: uncased BERT large model\n",
        "#   cased_L-12_H-768_A-12: cased BERT large model\n",
        "BERT_MODEL = 'uncased_L-12_H-768_A-12' #@param {type:\"string\"}\n",
        "BERT_PRETRAINED_DIR = 'gs://cloud-tpu-checkpoints/bert/' + BERT_MODEL\n",
        "print('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))\n",
        "!gsutil ls $BERT_PRETRAINED_DIR"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "***** BERT pretrained directory: gs://cloud-tpu-checkpoints/bert/uncased_L-12_H-768_A-12 *****\n",
            "gs://cloud-tpu-checkpoints/bert/uncased_L-12_H-768_A-12/bert_config.json\n",
            "gs://cloud-tpu-checkpoints/bert/uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001\n",
            "gs://cloud-tpu-checkpoints/bert/uncased_L-12_H-768_A-12/bert_model.ckpt.index\n",
            "gs://cloud-tpu-checkpoints/bert/uncased_L-12_H-768_A-12/bert_model.ckpt.meta\n",
            "gs://cloud-tpu-checkpoints/bert/uncased_L-12_H-768_A-12/checkpoint\n",
            "gs://cloud-tpu-checkpoints/bert/uncased_L-12_H-768_A-12/vocab.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NhO-sR6VVpZF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "LAYERS = [-1,-2]\n",
        "NUM_TPU_CORES = 8\n",
        "MAX_SEQ_LENGTH = 240\n",
        "BERT_CONFIG = BERT_PRETRAINED_DIR + '/bert_config.json'\n",
        "CHKPT_DIR = BERT_PRETRAINED_DIR + '/bert_model.ckpt'\n",
        "VOCAB_FILE = BERT_PRETRAINED_DIR + '/vocab.txt'\n",
        "INIT_CHECKPOINT = BERT_PRETRAINED_DIR + '/bert_model.ckpt'\n",
        "BATCH_SIZE = 64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lYe4ptTEXUx3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class InputExample(object):\n",
        "\n",
        "  def __init__(self, unique_id, text_a, text_b=None):\n",
        "    self.unique_id = unique_id\n",
        "    self.text_a = text_a\n",
        "    self.text_b = text_b"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_lmchkvCXaMM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class InputFeatures(object):\n",
        "  \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "  def __init__(self, unique_id, tokens, input_ids, input_mask, input_type_ids):\n",
        "    self.unique_id = unique_id\n",
        "    self.tokens = tokens\n",
        "    self.input_ids = input_ids\n",
        "    self.input_mask = input_mask\n",
        "    self.input_type_ids = input_type_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CUtxrVoRXch8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def input_fn_builder(features, seq_length):\n",
        "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
        "\n",
        "  all_unique_ids = []\n",
        "  all_input_ids = []\n",
        "  all_input_mask = []\n",
        "  all_input_type_ids = []\n",
        "\n",
        "  for feature in features:\n",
        "    all_unique_ids.append(feature.unique_id)\n",
        "    all_input_ids.append(feature.input_ids)\n",
        "    all_input_mask.append(feature.input_mask)\n",
        "    all_input_type_ids.append(feature.input_type_ids)\n",
        "\n",
        "  def input_fn(params):\n",
        "    \"\"\"The actual input function.\"\"\"\n",
        "    batch_size = params[\"batch_size\"]\n",
        "\n",
        "    num_examples = len(features)\n",
        "\n",
        "    # This is for demo purposes and does NOT scale to large data sets. We do\n",
        "    # not use Dataset.from_generator() because that uses tf.py_func which is\n",
        "    # not TPU compatible. The right way to load data is with TFRecordReader.\n",
        "    d = tf.data.Dataset.from_tensor_slices({\n",
        "        \"unique_ids\":\n",
        "            tf.constant(all_unique_ids, shape=[num_examples], dtype=tf.int32),\n",
        "        \"input_ids\":\n",
        "            tf.constant(\n",
        "                all_input_ids, shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"input_mask\":\n",
        "            tf.constant(\n",
        "                all_input_mask,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "        \"input_type_ids\":\n",
        "            tf.constant(\n",
        "                all_input_type_ids,\n",
        "                shape=[num_examples, seq_length],\n",
        "                dtype=tf.int32),\n",
        "    })\n",
        "\n",
        "    d = d.batch(batch_size=batch_size, drop_remainder=False)\n",
        "    return d\n",
        "\n",
        "  return input_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UwJM8FOaXfdV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model_fn_builder(bert_config, init_checkpoint, layer_indexes, use_tpu,\n",
        "                     use_one_hot_embeddings):\n",
        "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
        "\n",
        "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
        "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
        "\n",
        "    unique_ids = features[\"unique_ids\"]\n",
        "    input_ids = features[\"input_ids\"]\n",
        "    input_mask = features[\"input_mask\"]\n",
        "    input_type_ids = features[\"input_type_ids\"]\n",
        "\n",
        "    model = modeling.BertModel(\n",
        "        config=bert_config,\n",
        "        is_training=False,\n",
        "        input_ids=input_ids,\n",
        "        input_mask=input_mask,\n",
        "        token_type_ids=input_type_ids,\n",
        "        use_one_hot_embeddings=use_one_hot_embeddings)\n",
        "\n",
        "    if mode != tf.estimator.ModeKeys.PREDICT:\n",
        "      raise ValueError(\"Only PREDICT modes are supported: %s\" % (mode))\n",
        "\n",
        "    tvars = tf.trainable_variables()\n",
        "    scaffold_fn = None\n",
        "    (assignment_map,\n",
        "     initialized_variable_names) = modeling.get_assignment_map_from_checkpoint(\n",
        "         tvars, init_checkpoint)\n",
        "    if use_tpu:\n",
        "\n",
        "      def tpu_scaffold():\n",
        "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "        return tf.train.Scaffold()\n",
        "\n",
        "      scaffold_fn = tpu_scaffold\n",
        "    else:\n",
        "      tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
        "\n",
        "    #tf.logging.info(\"**** Trainable Variables ****\")\n",
        "    #for var in tvars:\n",
        "      #init_string = \"\"\n",
        "      #if var.name in initialized_variable_names:\n",
        "        #init_string = \", *INIT_FROM_CKPT*\"\n",
        "      #tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
        "                      #init_string)\n",
        "    \n",
        "    all_layers = model.get_all_encoder_layers()\n",
        "\n",
        "    predictions = {\n",
        "        \"unique_id\": unique_ids,\n",
        "    }\n",
        "\n",
        "    for (i, layer_index) in enumerate(layer_indexes):\n",
        "      predictions[\"layer_output_%d\" % i] = all_layers[layer_index]\n",
        "\n",
        "    output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
        "        mode=mode, predictions=predictions, scaffold_fn=scaffold_fn)\n",
        "    return output_spec\n",
        "\n",
        "  return model_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZSUni3ZvXigE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def convert_examples_to_features(examples, seq_length, tokenizer):\n",
        "  \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
        "\n",
        "  features = []\n",
        "  for (ex_index, example) in enumerate(examples):\n",
        "    tokens_a = tokenizer.tokenize(example.text_a)\n",
        "\n",
        "    tokens_b = None\n",
        "    if example.text_b:\n",
        "      tokens_b = tokenizer.tokenize(example.text_b)\n",
        "\n",
        "    if tokens_b:\n",
        "      # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "      # length is less than the specified length.\n",
        "      # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "      _truncate_seq_pair(tokens_a, tokens_b, seq_length - 3)\n",
        "    else:\n",
        "      # Account for [CLS] and [SEP] with \"- 2\"\n",
        "      if len(tokens_a) > seq_length - 2:\n",
        "        tokens_a = tokens_a[0:(seq_length - 2)]\n",
        "\n",
        "    # The convention in BERT is:\n",
        "    # (a) For sequence pairs:\n",
        "    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "    #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
        "    # (b) For single sequences:\n",
        "    #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "    #  type_ids: 0     0   0   0  0     0 0\n",
        "    #\n",
        "    # Where \"type_ids\" are used to indicate whether this is the first\n",
        "    # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "    # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "    # embedding vector (and position vector). This is not *strictly* necessary\n",
        "    # since the [SEP] token unambiguously separates the sequences, but it makes\n",
        "    # it easier for the model to learn the concept of sequences.\n",
        "    #\n",
        "    # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "    # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "    # the entire model is fine-tuned.\n",
        "    tokens = []\n",
        "    input_type_ids = []\n",
        "    tokens.append(\"[CLS]\")\n",
        "    input_type_ids.append(0)\n",
        "    for token in tokens_a:\n",
        "      tokens.append(token)\n",
        "      input_type_ids.append(0)\n",
        "    tokens.append(\"[SEP]\")\n",
        "    input_type_ids.append(0)\n",
        "\n",
        "    if tokens_b:\n",
        "      for token in tokens_b:\n",
        "        tokens.append(token)\n",
        "        input_type_ids.append(1)\n",
        "      tokens.append(\"[SEP]\")\n",
        "      input_type_ids.append(1)\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "    # tokens are attended to.\n",
        "    input_mask = [1] * len(input_ids)\n",
        "\n",
        "    # Zero-pad up to the sequence length.\n",
        "    while len(input_ids) < seq_length:\n",
        "      input_ids.append(0)\n",
        "      input_mask.append(0)\n",
        "      input_type_ids.append(0)\n",
        "\n",
        "    assert len(input_ids) == seq_length\n",
        "    assert len(input_mask) == seq_length\n",
        "    assert len(input_type_ids) == seq_length\n",
        "\n",
        "    #if ex_index < 5:\n",
        "      #tf.logging.info(\"*** Example ***\")\n",
        "      #tf.logging.info(\"unique_id: %s\" % (example.unique_id))\n",
        "      #tf.logging.info(\"tokens: %s\" % \" \".join(\n",
        "          #[tokenization.printable_text(x) for x in tokens]))\n",
        "      #tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "      #tf.logging.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
        "      #tf.logging.info(\n",
        "          #\"input_type_ids: %s\" % \" \".join([str(x) for x in input_type_ids]))\n",
        "\n",
        "    features.append(\n",
        "        InputFeatures(\n",
        "            unique_id=example.unique_id,\n",
        "            tokens=tokens,\n",
        "            input_ids=input_ids,\n",
        "            input_mask=input_mask,\n",
        "            input_type_ids=input_type_ids))\n",
        "  return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pghkTBjfXlW0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "  \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "  # This is a simple heuristic which will always truncate the longer sequence\n",
        "  # one token at a time. This makes more sense than truncating an equal percent\n",
        "  # of tokens from each, since if one sequence is very short then each token\n",
        "  # that's truncated likely contains more information than a longer sequence.\n",
        "  while True:\n",
        "    total_length = len(tokens_a) + len(tokens_b)\n",
        "    if total_length <= max_length:\n",
        "      break\n",
        "    if len(tokens_a) > len(tokens_b):\n",
        "      tokens_a.pop()\n",
        "    else:\n",
        "      tokens_b.pop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HIyhV3SvXrPe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_sequence(input_sentences):\n",
        "  examples = []\n",
        "  unique_id = 0\n",
        "  for sentence in input_sentences:\n",
        "    line = tokenization.convert_to_unicode(sentence)\n",
        "    examples.append(InputExample(unique_id=unique_id, text_a=line))\n",
        "    unique_id += 1\n",
        "  return examples"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ntg3W8-RXuul",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_features(input_text, dim=384):\n",
        "#   tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "  layer_indexes = LAYERS\n",
        "\n",
        "  bert_config = modeling.BertConfig.from_json_file(BERT_CONFIG)\n",
        "\n",
        "  tokenizer = tokenization.FullTokenizer(\n",
        "      vocab_file=VOCAB_FILE, do_lower_case=True)\n",
        "\n",
        "  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
        "  tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "  run_config = tf.contrib.tpu.RunConfig(\n",
        "      cluster=tpu_cluster_resolver,\n",
        "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "          num_shards=NUM_TPU_CORES,\n",
        "          per_host_input_for_training=is_per_host))\n",
        "\n",
        "  examples = read_sequence(input_text)\n",
        "\n",
        "  features = convert_examples_to_features(\n",
        "      examples=examples, seq_length=MAX_SEQ_LENGTH, tokenizer=tokenizer)\n",
        "\n",
        "  unique_id_to_feature = {}\n",
        "  for feature in features:\n",
        "    unique_id_to_feature[feature.unique_id] = feature\n",
        "\n",
        "  model_fn = model_fn_builder(\n",
        "      bert_config=bert_config,\n",
        "      init_checkpoint=INIT_CHECKPOINT,\n",
        "      layer_indexes=layer_indexes,\n",
        "      use_tpu=True,\n",
        "      use_one_hot_embeddings=True)\n",
        "\n",
        "  # If TPU is not available, this will fall back to normal Estimator on CPU\n",
        "  # or GPU.\n",
        "  estimator = tf.contrib.tpu.TPUEstimator(\n",
        "      use_tpu=True,\n",
        "      model_fn=model_fn,\n",
        "      config=run_config,\n",
        "      predict_batch_size=BATCH_SIZE,\n",
        "      train_batch_size=BATCH_SIZE)\n",
        "\n",
        "  input_fn = input_fn_builder(\n",
        "      features=features, seq_length=MAX_SEQ_LENGTH)\n",
        "\n",
        "  # Get features\n",
        "  for result in estimator.predict(input_fn, yield_single_examples=True):\n",
        "    unique_id = int(result[\"unique_id\"])\n",
        "    feature = unique_id_to_feature[unique_id]\n",
        "    output = collections.OrderedDict()\n",
        "    for (i, token) in enumerate(feature.tokens):\n",
        "      layers = []\n",
        "      for (j, layer_index) in enumerate(layer_indexes):\n",
        "        layer_output = result[\"layer_output_%d\" % j]\n",
        "        layer_output_flat = np.array([x for x in layer_output[i:(i + 1)].flat])\n",
        "        layers.append(layer_output_flat)\n",
        "      output[token] = sum(layers)[:dim]\n",
        "  \n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kgzRAkZWXxFb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "testembedding = get_features([\"你是谁\",\"我是狗\"])\n",
        "print(testembedding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9mRI2TC12r_m",
        "colab_type": "code",
        "outputId": "f14ca3f9-0cfe-4fb8-f424-dbae00010cab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]='2' \n",
        "from tensorflow.keras import backend as Backend\n",
        "from keras.models import Sequential\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "from keras.utils import plot_model\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "MAX_SENTS = 40\n",
        "VALIDATION_SPLIT = 0.1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "1ArnATzCgCSy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.utils import get_custom_objects\n",
        "\n",
        "\n",
        "class _BaseMultiHeadAttention(Layer):\n",
        "    \"\"\"\n",
        "    Base class for two types of Multi-head attention layers:\n",
        "    Self-attention and its more general form used in decoders (the one which\n",
        "    takes values and keys from the encoder).\n",
        "    \"\"\"\n",
        "    def __init__(self, num_heads: int, use_masking: bool,\n",
        "                 dropout: float = 0.0,\n",
        "                 compression_window_size: int = None,\n",
        "                 **kwargs):\n",
        "        \"\"\"\n",
        "        :param num_heads: number of attention heads\n",
        "        :param use_masking: when True, forbids the attention to see the further\n",
        "          elements in the sequence (particularly important in language\n",
        "          modelling).\n",
        "        :param dropout: dropout that should be applied to the attention\n",
        "          (after the softmax).\n",
        "        :param compression_window_size: an integer value >= 1 controlling\n",
        "          how much we should compress the attention. For more details,\n",
        "          read about memory-compressed self-attention in\n",
        "          \"Generating Wikipedia by summarizing long sequences\"\n",
        "          (https://arxiv.org/pdf/1801.10198.pdf).\n",
        "        :param kwargs: any extra arguments typical for a Keras layer,\n",
        "          such as name, etc.\n",
        "        \"\"\"\n",
        "        self.num_heads = num_heads\n",
        "        self.use_masking = use_masking\n",
        "        self.dropout = dropout\n",
        "        if (compression_window_size is not None\n",
        "                and compression_window_size <= 0):\n",
        "            assert ValueError(\n",
        "                f\"Too small compression window ({compression_window_size})\")\n",
        "        self.compression_window_size = compression_window_size\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config['num_heads'] = self.num_heads\n",
        "        config['use_masking'] = self.use_masking\n",
        "        config['dropout'] = self.dropout\n",
        "        config['compression_window_size'] = self.compression_window_size\n",
        "        return config\n",
        "\n",
        "    # noinspection PyAttributeOutsideInit\n",
        "    def build_output_params(self, d_model):\n",
        "        self.output_weights = self.add_weight(\n",
        "            name='output_weights',\n",
        "            shape=(d_model, d_model),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True)\n",
        "        if self.compression_window_size is not None:\n",
        "            self.k_conv_kernel = self.add_weight(\n",
        "                name='k_conv_kernel',\n",
        "                shape=(self.compression_window_size,\n",
        "                       d_model // self.num_heads,\n",
        "                       d_model // self.num_heads),\n",
        "                initializer='glorot_uniform',\n",
        "                trainable=True)\n",
        "            self.k_conv_bias = self.add_weight(\n",
        "                name='k_conv_bias',\n",
        "                shape=(d_model // self.num_heads,),\n",
        "                initializer='zeros',\n",
        "                trainable=True)\n",
        "            self.v_conv_kernel = self.add_weight(\n",
        "                name='v_conv_kernel',\n",
        "                shape=(self.compression_window_size,\n",
        "                       d_model // self.num_heads,\n",
        "                       d_model // self.num_heads),\n",
        "                initializer='glorot_uniform',\n",
        "                trainable=True)\n",
        "            self.v_conv_bias = self.add_weight(\n",
        "                name='v_conv_bias',\n",
        "                shape=(d_model // self.num_heads,),\n",
        "                initializer='zeros',\n",
        "                trainable=True)\n",
        "\n",
        "    def validate_model_dimensionality(self, d_model: int):\n",
        "        if d_model % self.num_heads != 0:\n",
        "            raise ValueError(\n",
        "                f'The size of the last dimension of the input '\n",
        "                f'({d_model}) must be evenly divisible by the number'\n",
        "                f'of the attention heads {self.num_heads}')\n",
        "\n",
        "    def attention(self, pre_q, pre_v, pre_k, out_seq_len: int, d_model: int,\n",
        "                  training=None):\n",
        "        \"\"\"\n",
        "        Calculates the output of the attention once the affine transformations\n",
        "        of the inputs are done. Here's the shapes of the arguments:\n",
        "        :param pre_q: (batch_size, q_seq_len, num_heads, d_model // num_heads)\n",
        "        :param pre_v: (batch_size, v_seq_len, num_heads, d_model // num_heads)\n",
        "        :param pre_k: (batch_size, k_seq_len, num_heads, d_model // num_heads)\n",
        "        :param out_seq_len: the length of the output sequence\n",
        "        :param d_model: dimensionality of the model (by the paper)\n",
        "        :param training: Passed by Keras. Should not be defined manually.\n",
        "          Optional scalar tensor indicating if we're in training\n",
        "          or inference phase.\n",
        "        \"\"\"\n",
        "        # shaping Q and V into (batch_size, num_heads, seq_len, d_model//heads)\n",
        "        q = K.permute_dimensions(pre_q, [0, 2, 1, 3])\n",
        "        v = K.permute_dimensions(pre_v, [0, 2, 1, 3])\n",
        "\n",
        "        if self.compression_window_size is None:\n",
        "            k_transposed = K.permute_dimensions(pre_k, [0, 2, 3, 1])\n",
        "        else:\n",
        "            # Memory-compressed attention described in paper\n",
        "            # \"Generating Wikipedia by Summarizing Long Sequences\"\n",
        "            # (https://arxiv.org/pdf/1801.10198.pdf)\n",
        "            # It compresses keys and values using 1D-convolution which reduces\n",
        "            # the size of Q * K_transposed from roughly seq_len^2\n",
        "            # to convoluted_seq_len^2. If we use strided convolution with\n",
        "            # window size = 3 and stride = 3, memory requirements of such\n",
        "            # memory-compressed attention will be 9 times smaller than\n",
        "            # that of the original version.\n",
        "            if self.use_masking:\n",
        "                raise NotImplementedError(\n",
        "                    \"Masked memory-compressed attention has not \"\n",
        "                    \"been implemented yet\")\n",
        "            k = K.permute_dimensions(pre_k, [0, 2, 1, 3])\n",
        "            k, v = [\n",
        "                K.reshape(\n",
        "                    # Step 3: Return the result to its original dimensions\n",
        "                    # (batch_size, num_heads, seq_len, d_model//heads)\n",
        "                    K.bias_add(\n",
        "                        # Step 3: ... and add bias\n",
        "                        K.conv1d(\n",
        "                            # Step 2: we \"compress\" K and V using strided conv\n",
        "                            K.reshape(\n",
        "                                # Step 1: we reshape K and V to\n",
        "                                # (batch + num_heads,  seq_len, d_model//heads)\n",
        "                                item,\n",
        "                                (-1,\n",
        "                                 K.int_shape(item)[-2],\n",
        "                                 d_model // self.num_heads)),\n",
        "                            kernel,\n",
        "                            strides=self.compression_window_size,\n",
        "                            padding='valid', data_format='channels_last'),\n",
        "                        bias,\n",
        "                        data_format='channels_last'),\n",
        "                    # new shape\n",
        "                    K.concatenate([\n",
        "                        K.shape(item)[:2],\n",
        "                        [-1, d_model // self.num_heads]]))\n",
        "                for item, kernel, bias in (\n",
        "                    (k, self.k_conv_kernel, self.k_conv_bias),\n",
        "                    (v, self.v_conv_kernel, self.v_conv_bias))]\n",
        "            k_transposed = K.permute_dimensions(k, [0, 1, 3, 2])\n",
        "        # shaping K into (batch_size, num_heads, d_model//heads, seq_len)\n",
        "        # for further matrix multiplication\n",
        "        sqrt_d = K.constant(np.sqrt(d_model // self.num_heads),\n",
        "                            dtype=K.floatx())\n",
        "        q_shape = K.int_shape(q)\n",
        "        k_t_shape = K.int_shape(k_transposed)\n",
        "        v_shape = K.int_shape(v)\n",
        "        # before performing batch_dot all tensors are being converted to 3D\n",
        "        # shape (batch_size * num_heads, rows, cols) to make sure batch_dot\n",
        "        # performs identically on all backends\n",
        "        attention_heads = K.reshape(\n",
        "            K.batch_dot(\n",
        "                self.apply_dropout_if_needed(\n",
        "                    K.softmax(\n",
        "                        self.mask_attention_if_needed(\n",
        "                            K.batch_dot(\n",
        "                                K.reshape(q, (-1,) + q_shape[-2:]),\n",
        "                                K.reshape(k_transposed,\n",
        "                                          (-1,) + k_t_shape[-2:]))\n",
        "                            / sqrt_d)),\n",
        "                    training=training),\n",
        "                K.reshape(v, (-1,) + v_shape[-2:])),\n",
        "            (-1, self.num_heads, q_shape[-2], v_shape[-1]))\n",
        "        attention_heads_merged = K.reshape(\n",
        "            K.permute_dimensions(attention_heads, [0, 2, 1, 3]),\n",
        "            (-1, d_model))\n",
        "        attention_out = K.reshape(\n",
        "            K.dot(attention_heads_merged, self.output_weights),\n",
        "            (-1, out_seq_len, d_model))\n",
        "        return attention_out\n",
        "\n",
        "    def apply_dropout_if_needed(self, attention_softmax, training=None):\n",
        "        if 0.0 < self.dropout < 1.0:\n",
        "            def dropped_softmax():\n",
        "                return K.dropout(attention_softmax, self.dropout)\n",
        "\n",
        "            return K.in_train_phase(dropped_softmax, attention_softmax,\n",
        "                                    training=training)\n",
        "        return attention_softmax\n",
        "\n",
        "    def mask_attention_if_needed(self, dot_product):\n",
        "        \"\"\"\n",
        "        Makes sure that (when enabled) each position\n",
        "        (of a decoder's self-attention) cannot attend to subsequent positions.\n",
        "        This is achieved by assigning -inf (or some large negative number)\n",
        "        to all invalid connections. Later softmax will turn them into zeros.\n",
        "        We need this to guarantee that decoder's predictions are based\n",
        "        on what has happened before the position, not after.\n",
        "        The method does nothing if masking is turned off.\n",
        "        :param dot_product: scaled dot-product of Q and K after reshaping them\n",
        "        to 3D tensors (batch * num_heads, rows, cols)\n",
        "        \"\"\"\n",
        "        if not self.use_masking:\n",
        "            return dot_product\n",
        "        last_dims = K.int_shape(dot_product)[-2:]\n",
        "        low_triangle_ones = (\n",
        "            np.tril(np.ones(last_dims))\n",
        "            # to ensure proper broadcasting\n",
        "            .reshape((1,) + last_dims))\n",
        "        inverse_low_triangle = 1 - low_triangle_ones\n",
        "        close_to_negative_inf = -1e9\n",
        "        result = (\n",
        "            K.constant(low_triangle_ones, dtype=K.floatx()) * dot_product +\n",
        "            K.constant(close_to_negative_inf * inverse_low_triangle))\n",
        "        return result\n",
        "\n",
        "\n",
        "class MultiHeadAttention(_BaseMultiHeadAttention):\n",
        "    \"\"\"\n",
        "    Multi-head attention which can use two inputs:\n",
        "    First: from the encoder - it's used to project the keys and the values\n",
        "    Second: from the decoder - used to project the queries.\n",
        "    \"\"\"\n",
        "\n",
        "    # noinspection PyAttributeOutsideInit\n",
        "    def build(self, input_shape):\n",
        "        if not (isinstance(input_shape, list) and len(input_shape) == 2):\n",
        "            raise ValueError(\n",
        "                'You must call this layer passing a list of two tensors'\n",
        "                '(for keys/values and queries)')\n",
        "        values_dim, query_dim = input_shape[0][-1], input_shape[1][-1]\n",
        "        if query_dim != values_dim:\n",
        "            raise ValueError(\n",
        "                f'Both keys/value and query inputs must be '\n",
        "                f'of the same dimensionality, instead of '\n",
        "                f'{values_dim} and {query_dim}.')\n",
        "        d_model = query_dim\n",
        "        self.validate_model_dimensionality(d_model)\n",
        "        # These weights are concatenated matrices W_k and W_v which\n",
        "        # are, in turn, concatenated W matrices of keys, and values\n",
        "        # for each of the heads. So, essentially it's a concatenation of\n",
        "        # W_k1, W_k2,..., W_kh, W_v1, W_v2,..., W_vh\n",
        "        # for all h heads.\n",
        "        self.kv_weights = self.add_weight(\n",
        "            name='kv_weights', shape=(d_model, d_model * 2),\n",
        "            initializer='glorot_uniform', trainable=True)\n",
        "        self.q_weights = self.add_weight(\n",
        "            name='q_weights', shape=(d_model, d_model),\n",
        "            initializer='glorot_uniform', trainable=True)\n",
        "        self.build_output_params(d_model)\n",
        "        return super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        if not (isinstance(inputs, list) and len(inputs) == 2):\n",
        "            raise ValueError(\n",
        "                'You can call this layer only with a list of two tensors '\n",
        "                '(for keys/values and queries)')\n",
        "        key_values_input, query_input = inputs\n",
        "        _, value_seq_len, d_model = K.int_shape(key_values_input)\n",
        "        query_seq_len = K.int_shape(inputs[1])[-2]\n",
        "        # The first thing we need to do is to perform affine transformations\n",
        "        # of the inputs to get the Queries, the Keys and the Values.\n",
        "        kv = K.dot(K.reshape(key_values_input, [-1, d_model]), self.kv_weights)\n",
        "        # splitting the keys, the values and the queries before further\n",
        "        # processing\n",
        "        pre_k, pre_v = [\n",
        "            K.reshape(\n",
        "                # K.slice(kv, (0, i * d_model), (-1, d_model)),\n",
        "                kv[:, i * d_model: (i + 1) * d_model],\n",
        "                (-1, value_seq_len,\n",
        "                 self.num_heads, d_model // self.num_heads))\n",
        "            for i in range(2)]\n",
        "        pre_q = K.reshape(\n",
        "            K.dot(K.reshape(query_input, [-1, d_model]), self.q_weights),\n",
        "            (-1, query_seq_len, self.num_heads, d_model // self.num_heads))\n",
        "        return self.attention(pre_q, pre_v, pre_k, query_seq_len, d_model,\n",
        "                              training=kwargs.get('training'))\n",
        "\n",
        "\n",
        "class MultiHeadSelfAttention(_BaseMultiHeadAttention):\n",
        "    \"\"\"\n",
        "    Multi-head self-attention for both encoders and decoders.\n",
        "    Uses only one input and has implementation which is better suited for\n",
        "    such use case that more general MultiHeadAttention class.\n",
        "    \"\"\"\n",
        "\n",
        "    # noinspection PyAttributeOutsideInit\n",
        "    def build(self, input_shape):\n",
        "        if not isinstance(input_shape, tuple):\n",
        "            raise ValueError('Invalid input')\n",
        "        d_model = input_shape[-1]\n",
        "        self.validate_model_dimensionality(d_model)\n",
        "        # These weights are concatenated matrices W_q, W_k and W_v which\n",
        "        # are, in turn, concatenated W matrices of keys, queries and values\n",
        "        # for each of the heads. So, essentially it's a concatenation of\n",
        "        # W_q1, W_q2,..., W_qh, W_k1, W_k2,..., W_kh, W_v1, W_v2,..., W_vh\n",
        "        # for all h heads.\n",
        "        self.qkv_weights = self.add_weight(\n",
        "            name='qkv_weights',\n",
        "            shape=(d_model, d_model * 3),  # * 3 for q, k and v\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True)\n",
        "        self.build_output_params(d_model)\n",
        "        return super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        if not K.is_tensor(inputs):\n",
        "            raise ValueError(\n",
        "                'The layer can be called only with one tensor as an argument')\n",
        "        _, seq_len, d_model = K.int_shape(inputs)\n",
        "        # The first thing we need to do is to perform affine transformations\n",
        "        # of the inputs to get the Queries, the Keys and the Values.\n",
        "        qkv = K.dot(K.reshape(inputs, [-1, d_model]), self.qkv_weights)\n",
        "        # splitting the keys, the values and the queries before further\n",
        "        # processing\n",
        "        pre_q, pre_k, pre_v = [\n",
        "            K.reshape(\n",
        "                # K.slice(qkv, (0, i * d_model), (-1, d_model)),\n",
        "                qkv[:, i * d_model:(i + 1) * d_model],\n",
        "                (-1, seq_len, self.num_heads, d_model // self.num_heads))\n",
        "            for i in range(3)]\n",
        "        attention_out = self.attention(pre_q, pre_v, pre_k, seq_len, d_model,\n",
        "                                       training=kwargs.get('training'))\n",
        "        return attention_out\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "\n",
        "get_custom_objects().update({\n",
        "    'MultiHeadSelfAttention': MultiHeadSelfAttention,\n",
        "    'MultiHeadAttention': MultiHeadAttention,\n",
        "})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "p9ssOyHygo27",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def positional_signal(hidden_size: int, length: int,\n",
        "                      min_timescale: float = 1.0, max_timescale: float = 1e4):\n",
        "    \"\"\"\n",
        "    Helper function, constructing basic positional encoding.\n",
        "    The code is partially based on implementation from Tensor2Tensor library\n",
        "    https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/layers/common_attention.py\n",
        "    \"\"\"\n",
        "\n",
        "    if hidden_size % 2 != 0:\n",
        "        raise ValueError(\n",
        "            f\"The hidden dimension of the model must be divisible by 2.\"\n",
        "            f\"Currently it is {hidden_size}\")\n",
        "    position = K.arange(0, length, dtype=K.floatx())\n",
        "    num_timescales = hidden_size // 2\n",
        "    log_timescale_increment = K.constant(\n",
        "        (np.log(float(max_timescale) / float(min_timescale)) /\n",
        "         (num_timescales - 1)),\n",
        "        dtype=K.floatx())\n",
        "    inv_timescales = (\n",
        "            min_timescale *\n",
        "            K.exp(K.arange(num_timescales, dtype=K.floatx()) *\n",
        "                  -log_timescale_increment))\n",
        "    scaled_time = K.expand_dims(position, 1) * K.expand_dims(inv_timescales, 0)\n",
        "    signal = K.concatenate([K.sin(scaled_time), K.cos(scaled_time)], axis=1)\n",
        "    return K.expand_dims(signal, axis=0)\n",
        "\n",
        "\n",
        "class AddPositionalEncoding(Layer):\n",
        "    \"\"\"\n",
        "    Injects positional encoding signal described in section 3.5 of the original\n",
        "    paper \"Attention is all you need\". Also a base class for more complex\n",
        "    coordinate encoding described in \"Universal Transformers\".\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, min_timescale: float = 1.0,\n",
        "                 max_timescale: float = 1.0e4, **kwargs):\n",
        "        self.min_timescale = min_timescale\n",
        "        self.max_timescale = max_timescale\n",
        "        self.signal = None\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config['min_timescale'] = self.min_timescale\n",
        "        config['max_timescale'] = self.max_timescale\n",
        "        return config\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        _, length, hidden_size = input_shape\n",
        "        self.signal = positional_signal(\n",
        "            hidden_size, length, self.min_timescale, self.max_timescale)\n",
        "        return super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        return inputs + self.signal\n",
        "\n",
        "\n",
        "class AddCoordinateEncoding(AddPositionalEncoding):\n",
        "    \"\"\"\n",
        "    Implements coordinate encoding described in section 2.1\n",
        "    of \"Universal Transformers\" (https://arxiv.org/abs/1807.03819).\n",
        "    In other words, injects two signals at once: current position in\n",
        "    the sequence, and current step (vertically) in the transformer model.\n",
        "    \"\"\"\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super().build(input_shape)\n",
        "        _, length, hidden_size = input_shape\n",
        "\n",
        "    def call(self, inputs, step=None, **kwargs):\n",
        "        if step is None:\n",
        "            raise ValueError(\"Please, provide current Transformer's step\"\n",
        "                             \"using 'step' keyword argument.\")\n",
        "        pos_encoded_added = super().call(inputs, **kwargs)\n",
        "        step_signal = K.expand_dims(self.signal[:, step, :], axis=1)\n",
        "        return pos_encoded_added + step_signal\n",
        "\n",
        "\n",
        "class TransformerCoordinateEmbedding(Layer):\n",
        "    \"\"\"\n",
        "    Represents trainable positional embeddings for the Transformer model:\n",
        "\n",
        "    1. word position embeddings - one for each position in the sequence.\n",
        "    2. depth embeddings - one for each block of the model\n",
        "\n",
        "    Calling the layer with the Transformer's input will return a new input\n",
        "    with those embeddings added.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, max_transformer_depth: int, **kwargs):\n",
        "        self.max_depth = max_transformer_depth\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config['max_transformer_depth'] = self.max_depth\n",
        "        return config\n",
        "\n",
        "    # noinspection PyAttributeOutsideInit\n",
        "    def build(self, input_shape):\n",
        "        sequence_length, d_model = input_shape[-2:]\n",
        "        self.word_position_embeddings = self.add_weight(\n",
        "            shape=(sequence_length, d_model),\n",
        "            initializer='uniform',\n",
        "            name='word_position_embeddings',\n",
        "            trainable=True)\n",
        "        self.depth_embeddings = self.add_weight(\n",
        "            shape=(self.max_depth, d_model),\n",
        "            initializer='uniform',\n",
        "            name='depth_position_embeddings',\n",
        "            trainable=True)\n",
        "        super().build(input_shape)\n",
        "\n",
        "    def call(self, inputs, **kwargs):\n",
        "        depth = kwargs.get('step')\n",
        "        if depth is None:\n",
        "            raise ValueError(\"Please, provide current Transformer's step\"\n",
        "                             \"using 'step' keyword argument.\")\n",
        "        result = inputs + self.word_position_embeddings\n",
        "        if depth is not None:\n",
        "            result = result + self.depth_embeddings[depth]\n",
        "        return result\n",
        "\n",
        "\n",
        "get_custom_objects().update({\n",
        "    'TransformerCoordinateEmbedding': TransformerCoordinateEmbedding,\n",
        "    'AddCoordinateEncoding': AddCoordinateEncoding,\n",
        "    'AddPositionalEncoding': AddCoordinateEncoding,\n",
        "})\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DHKIv33I2y1o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def Upstream(text):\n",
        "    \"\"\"generating sentence representation.\n",
        "    supported by bert-as-service.\n",
        "    source: https://bert-as-service.readthedocs.io\"\"\"\n",
        "    text = text.split('#')\n",
        "    vec = get_features(text)\n",
        "    vec_list = list(vec.values())\n",
        "    vec_array = np.asarray(vec_list)\n",
        "    mean_array = np.mean(vec_array,axis=0)\n",
        "    mean_array = mean_array.reshape(1,384)\n",
        "    pad = np.zeros((MAX_SENTS, 384))\n",
        "    pad[:mean_array.shape[0], :mean_array.shape[1]] = mean_array\n",
        "    pad = pad.reshape((1, MAX_SENTS, 384))\n",
        "    sen_vecs = np.array(pad).copy()\n",
        "    print(\"generating sen representation, shape=\" + str(np.shape(sen_vecs)))\n",
        "    return sen_vecs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZGN1KNyb23r1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('(1) pre-processing train texts...')\n",
        "train_texts = open('forbert/xa.txt', encoding='utf-8').read().split('\\n')\n",
        "\n",
        "train_docs = np.empty((0, MAX_SENTS, 384))\n",
        "for t_line in train_texts:\n",
        "    t_line = Upstream(t_line)\n",
        "    train_docs = np.append(arr=train_docs, values=t_line, axis=0)\n",
        "    \n",
        "print(' train_docs finished! shape is:'+str(np.shape(train_docs)))\n",
        "tf_train=tf.convert_to_tensor(train_docs, dtype=tf.float32)\n",
        "\n",
        "print('(2) pre-processing val texts...')\n",
        "val_texts = open('forbert/xb.txt', encoding='utf-8').read().split('\\n')\n",
        "\n",
        "val_docs = np.empty((0, MAX_SENTS, 384))\n",
        "for v_line in val_texts:\n",
        "    v_line = Upstream(v_line)\n",
        "    print(\"now val_docs shape=\" + str(np.shape(val_docs)))\n",
        "    val_docs = np.append(arr=val_docs, values=v_line, axis=0)\n",
        "\n",
        "print(' val_docs finished! shape is:'+str(np.shape(val_docs)))\n",
        "tf_val=tf.convert_to_tensor(val_docs, dtype=tf.float32)\n",
        "\n",
        "\n",
        "train_labels = open('forbert/ya.txt', encoding='utf-8').read().split('\\n')\n",
        "val_labels = open('forbert/yb.txt', encoding='utf-8').read().split('\\n')\n",
        "train_labels = to_categorical(np.asarray(train_labels))\n",
        "val_labels = to_categorical(np.asarray(val_labels))\n",
        "\n",
        "print('(3) building downstream model...')\n",
        "doc_input = Input(shape=(MAX_SENTS, 384), dtype='float32')\n",
        "pos = AddPositionalEncoding()(doc_input)\n",
        "doc_encoder = MultiHeadSelfAttention(num_heads=1, use_masking=False)(pos)\n",
        "flat = Flatten()(doc_encoder)\n",
        "dense = Dense(192, activation='relu')(flat)\n",
        "drop = Dropout(0.1)(dense)\n",
        "pred = Dense(2, activation='sigmoid')(drop)\n",
        "model = Model(doc_input, pred)\n",
        "model.summary()\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['acc'])\n",
        "print (model.metrics_names)\n",
        "model.fit(tf_train, train_labels, validation_data=(tf_val, val_labels), epochs=20, steps_per_epoch=64, validation_steps=32)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
